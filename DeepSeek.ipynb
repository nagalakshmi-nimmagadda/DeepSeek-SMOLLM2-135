{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7a2f14bd080416fb7d8f1a36eb457e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daefbc2c7bc244a1851341e6926800b4",
              "IPY_MODEL_a2664194d3af4fd8bc66073ffe9429aa",
              "IPY_MODEL_42e2b4d96649470080d43d657d0cd73a"
            ],
            "layout": "IPY_MODEL_f5495b4cd5754b6f9fc4ddd55f2e1b3f"
          }
        },
        "daefbc2c7bc244a1851341e6926800b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c76beb614ef4446ebe0b26d8d7cb8bcc",
            "placeholder": "​",
            "style": "IPY_MODEL_db7ffb7f3eda408c88d15378146c1d30",
            "value": "Resolving data files: 100%"
          }
        },
        "a2664194d3af4fd8bc66073ffe9429aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489089ac42dc45989ebd97f7cb29fb81",
            "max": 104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a1c895a6e664daa92469527cf47b25e",
            "value": 104
          }
        },
        "42e2b4d96649470080d43d657d0cd73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_133d7b62592848d2af297888d48ea4f8",
            "placeholder": "​",
            "style": "IPY_MODEL_564f61783b5648f5959be1af21c4beac",
            "value": " 104/104 [00:00&lt;00:00, 419.68it/s]"
          }
        },
        "f5495b4cd5754b6f9fc4ddd55f2e1b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c76beb614ef4446ebe0b26d8d7cb8bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db7ffb7f3eda408c88d15378146c1d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "489089ac42dc45989ebd97f7cb29fb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1c895a6e664daa92469527cf47b25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "133d7b62592848d2af297888d48ea4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564f61783b5648f5959be1af21c4beac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e299b591afb74b608ab57c40580e27fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2272006ff0754cc8b97614770e1db15b",
              "IPY_MODEL_432b711be29844489b5793246e32b349",
              "IPY_MODEL_26d650c3516c4248bebea58b4a7e58ab"
            ],
            "layout": "IPY_MODEL_e8d5a28017e84995992155da66439504"
          }
        },
        "2272006ff0754cc8b97614770e1db15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d8f0067356450988b2e18a80650725",
            "placeholder": "​",
            "style": "IPY_MODEL_69b91aaff027418fa7837d7e8dc5b79c",
            "value": "Resolving data files: 100%"
          }
        },
        "432b711be29844489b5793246e32b349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c907dde691d49479c6ea346bc4898ee",
            "max": 104,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_566a89499ec34f8ebe189d6a2ad6ddb3",
            "value": 104
          }
        },
        "26d650c3516c4248bebea58b4a7e58ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e723513fbe94723a35abb22e30fef57",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8e09fda9fa4313ae2cb2cb516210e9",
            "value": " 104/104 [00:00&lt;00:00, 2179.95it/s]"
          }
        },
        "e8d5a28017e84995992155da66439504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24d8f0067356450988b2e18a80650725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b91aaff027418fa7837d7e8dc5b79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c907dde691d49479c6ea346bc4898ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566a89499ec34f8ebe189d6a2ad6ddb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e723513fbe94723a35abb22e30fef57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8e09fda9fa4313ae2cb2cb516210e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qg26z_HUFSwj"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets tqdm huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this at the start of the notebook for better formatting\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OSXIHKwwFW73",
        "outputId": "b714840e-7f91-4668-a848-0716b1a97827"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# Create project directories\n",
        "PROJECT_DIR = \"deepseek-training\"  # Changed from Google Drive path\n",
        "CHECKPOINTS_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
        "LOGS_DIR = os.path.join(PROJECT_DIR, \"logs\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for dir_path in [PROJECT_DIR, CHECKPOINTS_DIR, LOGS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "log_file = os.path.join(LOGS_DIR, f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        norm = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * norm * self.weight\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_length: int = 2048, theta: float = 10000.0):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float)\n",
        "        freqs = torch.exp(\n",
        "            -torch.arange(0, dim, 2, dtype=torch.float) * (math.log(theta) / dim)\n",
        "        )\n",
        "        angles = position.unsqueeze(1) * freqs.unsqueeze(0)\n",
        "        self.register_buffer(\"cos\", angles.cos())\n",
        "        self.register_buffer(\"sin\", angles.sin())\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None) -> torch.Tensor:\n",
        "        if seq_len is None:\n",
        "            seq_len = x.size(1)\n",
        "\n",
        "        cos = self.cos[:seq_len]\n",
        "        sin = self.sin[:seq_len]\n",
        "\n",
        "        cos = cos.view(1, seq_len, 1, -1)\n",
        "        sin = sin.view(1, seq_len, 1, -1)\n",
        "\n",
        "        x_even = x[..., ::2]\n",
        "        x_odd = x[..., 1::2]\n",
        "\n",
        "        rotated = torch.cat([\n",
        "            x_even * cos - x_odd * sin,\n",
        "            x_even * sin + x_odd * cos\n",
        "        ], dim=-1)\n",
        "\n",
        "        return rotated\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, num_key_value_heads: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, (d_model // num_heads) * num_key_value_heads, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, (d_model // num_heads) * num_key_value_heads, bias=False)\n",
        "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)\n",
        "\n",
        "        if self.num_key_value_heads != self.num_heads:\n",
        "            k = k.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=2)\n",
        "            v = v.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=2)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return self.o_proj(out)\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, num_experts: int = 4, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Initialize experts\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, d_ff),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(d_ff, d_model),\n",
        "                nn.Dropout(dropout)\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Router\n",
        "        self.router = nn.Linear(d_model, num_experts)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Get routing probabilities\n",
        "        route_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
        "        route_probs = F.softmax(route_logits, dim=-1)\n",
        "\n",
        "        # Process each token through experts\n",
        "        combined_output = torch.zeros_like(x)\n",
        "        for i in range(self.num_experts):\n",
        "            # Get expert outputs\n",
        "            expert_output = self.experts[i](x)\n",
        "            # Weight by routing probability\n",
        "            combined_output += expert_output * route_probs[..., i:i+1]\n",
        "\n",
        "        return combined_output\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, num_key_value_heads: int, d_ff: int, num_experts: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, num_key_value_heads, dropout)\n",
        "        self.moe = MoELayer(d_model, d_ff, num_experts, dropout)\n",
        "        self.norm1 = RMSNorm(d_model)\n",
        "        self.norm2 = RMSNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # Self-attention\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x, attention_mask)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        # MoE FFN\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.moe(x)\n",
        "        x = residual + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class DeepSeekLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 384,\n",
        "        num_layers: int = 20,\n",
        "        num_heads: int = 6,\n",
        "        d_ff: int = 1536,\n",
        "        num_experts: int = 4,\n",
        "        rope_theta: float = 10000.0,\n",
        "        max_seq_length: int = 2048,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.num_experts = num_experts\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_key_value_heads = max(1, num_heads // 3)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = RotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_seq_length=max_seq_length,\n",
        "            theta=rope_theta\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                num_key_value_heads=self.num_key_value_heads,\n",
        "                d_ff=d_ff,\n",
        "                num_experts=num_experts,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = RMSNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        x = self.pos_embedding(x)\n",
        "        x = x.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            for block in self.blocks:\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs)\n",
        "                    return custom_forward\n",
        "\n",
        "                x = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(block),\n",
        "                    x, attention_mask\n",
        "                )\n",
        "        else:\n",
        "            for block in self.blocks:\n",
        "                x = block(x, attention_mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"Enable gradient checkpointing\"\"\"\n",
        "        self.gradient_checkpointing = True\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"Disable gradient checkpointing\"\"\"\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count and format model parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Count parameters by layer type\n",
        "    layer_params = {}\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        layer_type = name.split('.')[0]\n",
        "        layer_params[layer_type] = layer_params.get(layer_type, 0) + p.numel()\n",
        "\n",
        "    return total_params, layer_params\n",
        "\n",
        "def format_number(num):\n",
        "    \"\"\"Format large numbers with commas and calculate size in MB\"\"\"\n",
        "    return f\"{num:,} (~{num/1e6:.1f}M)\"\n",
        "\n",
        "def get_latest_checkpoint():\n",
        "    \"\"\"Find the latest valid checkpoint in the checkpoints directory\"\"\"\n",
        "    if not os.path.exists(CHECKPOINTS_DIR):\n",
        "        return None\n",
        "\n",
        "    checkpoints = [f for f in os.listdir(CHECKPOINTS_DIR)\n",
        "                  if (f.startswith('step_') or f.startswith('interrupt_step_'))\n",
        "                  and f.endswith('.pt')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "\n",
        "    # Extract step numbers and find latest valid checkpoint\n",
        "    valid_checkpoints = []\n",
        "    for f in checkpoints:\n",
        "        try:\n",
        "            checkpoint_path = os.path.join(CHECKPOINTS_DIR, f)\n",
        "            # Try to load checkpoint to verify integrity\n",
        "            torch.load(checkpoint_path, map_location='cpu')  # Load on CPU first to verify\n",
        "\n",
        "            if f.startswith('interrupt_step_'):\n",
        "                step = int(f.split('_')[2].split('.')[0])\n",
        "            else:\n",
        "                step = int(f.split('_')[1].split('.')[0])\n",
        "            valid_checkpoints.append((step, f))\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupted checkpoint {f}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not valid_checkpoints:\n",
        "        return None\n",
        "\n",
        "    # Get the checkpoint with highest step count\n",
        "    _, latest_file = max(valid_checkpoints, key=lambda x: x[0])\n",
        "    checkpoint_path = os.path.join(CHECKPOINTS_DIR, latest_file)\n",
        "    return checkpoint_path\n",
        "\n",
        "def save_checkpoint(model, optimizer, step, loss, tokens_processed, is_interrupt=False, is_final=False):\n",
        "    \"\"\"Save checkpoint with proper error handling\"\"\"\n",
        "    try:\n",
        "        if is_final:\n",
        "            filename = \"final_model.pt\"\n",
        "        elif is_interrupt:\n",
        "            filename = f\"interrupt_step_{step}.pt\"\n",
        "        else:\n",
        "            filename = f\"step_{step}.pt\"\n",
        "\n",
        "        checkpoint_path = os.path.join(CHECKPOINTS_DIR, filename)\n",
        "\n",
        "        # Save to temporary file first\n",
        "        temp_path = checkpoint_path + '.tmp'\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'tokens_processed': tokens_processed,\n",
        "        }, temp_path)\n",
        "\n",
        "        # If save was successful, rename to final filename\n",
        "        os.replace(temp_path, checkpoint_path)\n",
        "        return checkpoint_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint: {str(e)}\")\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "        return None\n",
        "\n",
        "def setup_tokenizer_and_dataset(max_length=256):\n",
        "    \"\"\"Setup tokenizer and dataset with progress tracking\"\"\"\n",
        "    print(\"\\nInitializing training components:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Initialize tokenizer with progress tracking\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"HuggingFaceTB/cosmo2-tokenizer\",\n",
        "        trust_remote_code=True,\n",
        "        use_fast=True  # Use fast tokenizer\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"✓ Tokenizer loaded successfully\")\n",
        "\n",
        "    # Load dataset with progress tracking\n",
        "    print(\"\\nLoading dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\n",
        "            \"HuggingFaceTB/smollm-corpus\",\n",
        "            name=\"cosmopedia-v2\",\n",
        "            streaming=True,\n",
        "            split=\"train\"\n",
        "        )\n",
        "\n",
        "        # Calculate approximate dataset size\n",
        "        dataset_size = dataset.dataset_size\n",
        "        sample_size = int(dataset_size * 0.3)\n",
        "        print(f\"✓ Dataset loaded successfully\")\n",
        "        print(f\"  - Total samples: {dataset_size:,}\")\n",
        "        print(f\"  - Using {sample_size:,} samples (30%)\")\n",
        "\n",
        "        # Take 30% of data with progress tracking\n",
        "        dataset = dataset.take(sample_size)\n",
        "        dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "        return tokenizer, dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def setup_logging():\n",
        "    \"\"\"Setup logging with both file and console handlers\"\"\"\n",
        "    # Create a unique session ID\n",
        "    session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Ensure directories exist\n",
        "    os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "    # Setup file handler to append to existing log file\n",
        "    log_file = os.path.join(LOGS_DIR, \"training_history.log\")\n",
        "    file_handler = logging.FileHandler(log_file, mode='a')\n",
        "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
        "\n",
        "    # Setup console handler with minimal output\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "    console_handler.setLevel(logging.WARNING)\n",
        "\n",
        "    # Configure root logger\n",
        "    logging.getLogger().handlers = []\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        handlers=[file_handler, console_handler]\n",
        "    )\n",
        "\n",
        "    # Create CSV progress file for this session\n",
        "    progress_file = os.path.join(LOGS_DIR, f\"progress_{session_id}.csv\")\n",
        "    os.makedirs(os.path.dirname(progress_file), exist_ok=True)\n",
        "    with open(progress_file, 'w') as f:\n",
        "        f.write(\"step,loss,tokens_processed,time,tokens_per_sec\\n\")\n",
        "\n",
        "    return log_file, progress_file, session_id\n",
        "\n",
        "def train(resume_training=True):\n",
        "    \"\"\"Train the model with support for resuming from checkpoints\"\"\"\n",
        "    # Setup logging first\n",
        "    log_file, progress_file, session_id = setup_logging()\n",
        "\n",
        "    # Log session start\n",
        "    logging.info(\"\\n\" + \"=\"*50)\n",
        "    logging.info(f\"Starting new training session: {session_id}\")\n",
        "\n",
        "    # Create directories if needed\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "    # Initialize components\n",
        "    tokenizer, dataset = setup_tokenizer_and_dataset()\n",
        "    dataset_iter = iter(dataset)\n",
        "\n",
        "    # Training parameters - optimized for speed\n",
        "    batch_size = 4  # Increased from 2\n",
        "    learning_rate = 3e-4\n",
        "    max_steps = 10000\n",
        "    save_every = 1000\n",
        "    log_every = 100\n",
        "    max_length = 256  # Keep this the same\n",
        "    grad_clip = 1.0\n",
        "    warmup_steps = 1000\n",
        "    gradient_accumulation_steps = 4  # Reduced from 8 (maintains same effective batch size)\n",
        "\n",
        "    # Model parameters - adjusted to target exactly 135M parameters\n",
        "    model = DeepSeekLM(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=512,        # Reduced from 576\n",
        "        num_layers=12,      # Adjusted from 10\n",
        "        num_heads=8,        # Reduced from 9\n",
        "        d_ff=2048,         # 4x d_model\n",
        "        num_experts=4,      # Reduced from 6\n",
        "        max_seq_length=max_length,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # Print model size details\n",
        "    total_params = sum(p.numel() for p in model.parameters())/1e6\n",
        "    print(\"\\nModel Configuration:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Parameters: {total_params:.2f}M\")\n",
        "    print(f\"Embedding dim: {model.d_model}\")\n",
        "    print(f\"Layers: {model.num_layers}\")\n",
        "    print(f\"Attention heads: {model.num_heads}\")\n",
        "    print(f\"FF dim: {model.d_ff}\")\n",
        "    print(f\"MoE experts: {model.num_experts}\")\n",
        "    print(f\"Sequence length: {max_length}\")\n",
        "    print(f\"Batch size: {batch_size} (effective: {batch_size * gradient_accumulation_steps})\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"✓ Gradient checkpointing enabled\")\n",
        "\n",
        "    # Set memory efficient attention\n",
        "    torch.backends.cuda.max_memory_split_size = None\n",
        "    torch.backends.cuda.max_memory_cached = None\n",
        "\n",
        "    # Empty CUDA cache before moving model to device\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Move model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize mixed precision training with lower precision\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Enable faster training options\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Data loading optimization\n",
        "    def get_next_batch(dataset_iter):\n",
        "        \"\"\"Get next batch of data with error handling\"\"\"\n",
        "        max_retries = 3\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                batch_texts = []\n",
        "                while len(batch_texts) < batch_size:\n",
        "                    try:\n",
        "                        sample = next(dataset_iter)\n",
        "                        if not isinstance(sample['text'], str):\n",
        "                            continue\n",
        "                        if len(sample['text'].strip()) < 10:  # Skip very short texts\n",
        "                            continue\n",
        "                        batch_texts.append(sample['text'])\n",
        "                    except StopIteration:\n",
        "                        dataset_iter = iter(dataset.shuffle())\n",
        "                        sample = next(dataset_iter)\n",
        "                        batch_texts.append(sample['text'])\n",
        "\n",
        "                # Process current batch\n",
        "                inputs = tokenizer(\n",
        "                    batch_texts,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    padding='max_length',\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "                return inputs, dataset_iter\n",
        "            except Exception as e:\n",
        "                if retry == max_retries - 1:\n",
        "                    raise Exception(f\"Failed to get batch after {max_retries} attempts: {str(e)}\")\n",
        "                print(f\"Retry {retry + 1}/{max_retries}: Error getting batch: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Initialize session\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        betas=(0.9, 0.95),\n",
        "        eps=1e-8,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # Add learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=max_steps,\n",
        "        eta_min=learning_rate/10\n",
        "    )\n",
        "\n",
        "    # Initialize training state\n",
        "    step = 0\n",
        "    tokens_processed = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Try to load checkpoint if resume_training is True\n",
        "    if resume_training:\n",
        "        checkpoint_path = get_latest_checkpoint()\n",
        "        if checkpoint_path:\n",
        "            print(f\"\\nFound valid checkpoint at {checkpoint_path}\")\n",
        "            try:\n",
        "                checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "                # Verify checkpoint contents\n",
        "                required_keys = ['model_state_dict', 'optimizer_state_dict', 'step', 'loss']\n",
        "                if not all(k in checkpoint for k in required_keys):\n",
        "                    raise ValueError(\"Checkpoint missing required keys\")\n",
        "\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "                # Move optimizer state to correct device\n",
        "                for state in optimizer.state.values():\n",
        "                    for k, v in state.items():\n",
        "                        if isinstance(v, torch.Tensor):\n",
        "                            state[k] = v.to(device)\n",
        "\n",
        "                step = checkpoint['step']\n",
        "                tokens_processed = checkpoint.get('tokens_processed', 0)\n",
        "                best_loss = checkpoint.get('loss', float('inf'))\n",
        "\n",
        "                # Update scheduler to correct step\n",
        "                for _ in range(step):\n",
        "                    scheduler.step()\n",
        "\n",
        "                print(f\"✓ Successfully resumed training from step {step}\")\n",
        "                print(f\"  - Previous loss: {best_loss:.4f}\")\n",
        "                print(f\"  - Tokens processed: {tokens_processed:,}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint: {str(e)}\")\n",
        "                print(\"Starting fresh training...\")\n",
        "                step = 0\n",
        "                tokens_processed = 0\n",
        "                best_loss = float('inf')\n",
        "        else:\n",
        "            print(\"No valid checkpoints found, starting fresh training\")\n",
        "            step = 0\n",
        "            tokens_processed = 0\n",
        "            best_loss = float('inf')\n",
        "    else:\n",
        "        print(\"Starting fresh training\")\n",
        "        step = 0\n",
        "        tokens_processed = 0\n",
        "        best_loss = float('inf')\n",
        "\n",
        "    # Get detailed parameter counts and log architecture details\n",
        "    total_params, layer_params = count_parameters(model)\n",
        "\n",
        "    logging.info(\"\\nModel Architecture Details:\")\n",
        "    logging.info(\"=\"*50)\n",
        "    logging.info(f\"Total Parameters: {format_number(total_params)}\")\n",
        "    logging.info(\"\\nParameters by layer:\")\n",
        "    for layer, params in layer_params.items():\n",
        "        logging.info(f\"- {layer}: {format_number(params)}\")\n",
        "\n",
        "    logging.info(\"\\nArchitecture Configuration:\")\n",
        "    logging.info(f\"- Model dimension: {model.d_model}\")\n",
        "    logging.info(f\"- Number of layers: {model.num_layers}\")\n",
        "    logging.info(f\"- Attention heads: {model.num_heads}\")\n",
        "    logging.info(f\"- KV heads: {model.num_key_value_heads} (MLHA)\")\n",
        "    logging.info(f\"- FF dimension: {model.d_ff}\")\n",
        "    logging.info(f\"- MoE experts: {model.num_experts}\")\n",
        "    logging.info(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    try:\n",
        "        print(\"\\nStarting training loop...\")\n",
        "        progress_bar = tqdm(\n",
        "            total=max_steps,\n",
        "            desc=\"Training\",\n",
        "            position=0,\n",
        "            leave=True,\n",
        "            ncols=100,\n",
        "            disable=True  # Disable tqdm bar, we'll use our own progress format\n",
        "        )\n",
        "\n",
        "        while step < max_steps:\n",
        "            try:\n",
        "                accumulated_loss = 0\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Gradient accumulation loop\n",
        "                for accum_step in range(gradient_accumulation_steps):\n",
        "                    # Get next batch with progress info\n",
        "                    if step == 0 and accum_step == 0:\n",
        "                        print(\"Getting first batch...\")\n",
        "\n",
        "                    inputs, dataset_iter = get_next_batch(dataset_iter)\n",
        "                    if step == 0 and accum_step == 0:\n",
        "                        print(\"✓ First batch processed successfully\")\n",
        "\n",
        "                    inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
        "\n",
        "                    # Training step with progress info\n",
        "                    if step == 0 and accum_step == 0:\n",
        "                        print(\"Starting first forward pass...\")\n",
        "\n",
        "                    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                        logits = model(inputs['input_ids'])\n",
        "                        shift_logits = logits[..., :-1, :].contiguous()\n",
        "                        shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
        "                        loss = F.cross_entropy(\n",
        "                            shift_logits.view(-1, tokenizer.vocab_size),\n",
        "                            shift_labels.view(-1)\n",
        "                        )\n",
        "                        loss = loss / gradient_accumulation_steps\n",
        "\n",
        "                    if step == 0 and accum_step == 0:\n",
        "                        print(f\"✓ First forward pass complete (loss: {loss.item():.4f})\")\n",
        "                        print(\"Starting first backward pass...\")\n",
        "\n",
        "                    # Scale loss and backward pass\n",
        "                    scaler.scale(loss).backward()\n",
        "                    accumulated_loss += loss.item()\n",
        "\n",
        "                    if step == 0 and accum_step == 0:\n",
        "                        print(\"✓ First backward pass complete\")\n",
        "\n",
        "                    # Memory cleanup\n",
        "                    del logits, shift_logits, shift_labels\n",
        "                    if step % 100 == 0:\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                # Optimizer step with gradient clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                if step < warmup_steps:\n",
        "                    lr = learning_rate * (step / warmup_steps)\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = lr\n",
        "                else:\n",
        "                    scheduler.step()\n",
        "\n",
        "                # Update counters\n",
        "                step += 1\n",
        "                tokens_processed += inputs['input_ids'].numel() * gradient_accumulation_steps\n",
        "                running_loss += accumulated_loss\n",
        "\n",
        "                # Print progress for every step\n",
        "                elapsed = time.time() - start_time\n",
        "                tokens_per_sec = tokens_processed / elapsed\n",
        "                gpu_mem_alloc = torch.cuda.memory_allocated() / 1024**2\n",
        "                gpu_mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "\n",
        "                progress_msg = (\n",
        "                    f\"\\rStep {step:5d}/{max_steps} ({step/max_steps*100:.1f}%) | \"\n",
        "                    f\"Loss: {accumulated_loss:.4f} | \"\n",
        "                    f\"Time: {elapsed:.1f}s | \"\n",
        "                    f\"Tokens/sec: {tokens_per_sec:.2f} | \"\n",
        "                    f\"Total Tokens: {tokens_processed:,} | \"\n",
        "                    f\"GPU Memory: {gpu_mem_alloc:.0f}MB/{gpu_mem_reserved:.0f}MB\"\n",
        "                )\n",
        "                print(progress_msg, end=\"\", flush=True)\n",
        "\n",
        "                # Log detailed stats every 100 steps\n",
        "                if step % log_every == 0:\n",
        "                    avg_loss = running_loss / log_every\n",
        "\n",
        "                    # Log to file\n",
        "                    logging.info(progress_msg)\n",
        "\n",
        "                    # Update CSV progress\n",
        "                    try:\n",
        "                        with open(progress_file, 'a') as f:\n",
        "                            f.write(f\"{step},{avg_loss},{tokens_processed},{elapsed},{tokens_per_sec}\\n\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"\\nWarning: Could not write to progress file: {str(e)}\")\n",
        "\n",
        "                    running_loss = 0.0\n",
        "                    print()  # New line after logging\n",
        "\n",
        "                # Save checkpoint every 1000 steps\n",
        "                if step % save_every == 0:\n",
        "                    saved_path = save_checkpoint(\n",
        "                        model, optimizer, step, accumulated_loss,\n",
        "                        tokens_processed, is_interrupt=False\n",
        "                    )\n",
        "                    if saved_path:\n",
        "                        logging.info(f\"Saved checkpoint to {saved_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during training step {step}: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Log completion\n",
        "        logging.info(\"\\nTraining completed successfully!\")\n",
        "        logging.info(f\"Total steps: {step}\")\n",
        "        logging.info(f\"Total tokens processed: {tokens_processed:,}\")\n",
        "        logging.info(f\"Best loss achieved: {best_loss:.4f}\")\n",
        "        logging.info(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        progress_bar.close()\n",
        "        logging.warning(\"\\nTraining interrupted! Saving checkpoint...\")\n",
        "        save_checkpoint(\n",
        "            model, optimizer, step, accumulated_loss,\n",
        "            tokens_processed, is_interrupt=True\n",
        "        )\n",
        "\n",
        "    return model, tokenizer, log_file\n",
        "\n",
        "def generate_samples(model, tokenizer, device, num_samples=5, max_new_tokens=100, temperature=1.0):\n",
        "    \"\"\"Generate text samples from the model\"\"\"\n",
        "    print(\"\\nGenerating samples:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model.eval()\n",
        "    prompts = [\n",
        "        \"The quantum mechanics of particles describes how\",\n",
        "        \"In computer science, neural networks can learn to\",\n",
        "        \"The theory of relativity fundamentally changed our understanding of\",\n",
        "        \"Machine learning algorithms have revolutionized\",\n",
        "        \"The structure of DNA contains information about\"\n",
        "    ]\n",
        "\n",
        "    # Generation parameters - tuned for better output\n",
        "    temperature = 1.0       # Higher temperature for more randomness\n",
        "    top_k = 100            # Increased for more variety\n",
        "    top_p = 0.95          # Slightly higher nucleus sampling threshold\n",
        "    repetition_penalty = 1.5  # Increased repetition penalty\n",
        "    min_length = 20       # Ensure minimum output length\n",
        "\n",
        "    for i, prompt in enumerate(prompts[:num_samples], 1):\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_ids = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)\n",
        "            output_sequence = input_ids.clone()\n",
        "\n",
        "            # Track generated tokens and their counts\n",
        "            token_counts = {}\n",
        "            generated_tokens = []\n",
        "\n",
        "            for _ in range(max_new_tokens):\n",
        "                outputs = model(output_sequence)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "                # Apply stronger repetition penalty based on token frequency\n",
        "                for token, count in token_counts.items():\n",
        "                    next_token_logits[0, token] /= (repetition_penalty ** count)\n",
        "\n",
        "                # Apply top-k filtering\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
        "\n",
        "                # Apply dynamic nucleus sampling\n",
        "                probs = torch.softmax(top_k_logits, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "                nucleus_mask = cumulative_probs < top_p\n",
        "                nucleus_mask[..., 1:] = nucleus_mask[..., :-1].clone()\n",
        "                nucleus_mask[..., 0] = True\n",
        "\n",
        "                # Apply stronger filtering for common tokens\n",
        "                filtered_logits = top_k_logits.masked_fill(~nucleus_mask, float('-inf'))\n",
        "                filtered_probs = torch.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "                next_token_idx = torch.multinomial(filtered_probs, num_samples=1)\n",
        "                next_token = top_k_indices.gather(-1, next_token_idx)\n",
        "\n",
        "                # Update token counts\n",
        "                token = next_token.item()\n",
        "                token_counts[token] = token_counts.get(token, 0) + 1\n",
        "\n",
        "                # Stop if we generate an EOS token and passed minimum length\n",
        "                if next_token.item() == tokenizer.eos_token_id and len(generated_tokens) >= min_length:\n",
        "                    break\n",
        "\n",
        "                # Add token and check for repetition\n",
        "                generated_tokens.append(token)\n",
        "                output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
        "\n",
        "                # Check for repetition patterns\n",
        "                if len(generated_tokens) >= 4:\n",
        "                    last_4 = generated_tokens[-4:]\n",
        "                    if len(set(last_4)) == 1 or (  # Same token repeated\n",
        "                        len(generated_tokens) >= 8 and\n",
        "                        generated_tokens[-4:] == generated_tokens[-8:-4]  # Repeating pattern\n",
        "                    ):\n",
        "                        break\n",
        "\n",
        "            generated_text = tokenizer.decode(output_sequence[0], skip_special_tokens=True)\n",
        "            print(f\"Generated: {generated_text}\")\n",
        "            print(\"-\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Update paths for Google Drive\n",
        "    global PROJECT_DIR, CHECKPOINTS_DIR, LOGS_DIR\n",
        "    PROJECT_DIR = \"/content/drive/MyDrive/deepseek-training\"\n",
        "    CHECKPOINTS_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
        "    LOGS_DIR = os.path.join(PROJECT_DIR, \"logs\")\n",
        "\n",
        "    # Create directories\n",
        "    for dir_path in [PROJECT_DIR, CHECKPOINTS_DIR, LOGS_DIR]:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "    # Set resume_training=False to force fresh training\n",
        "    model, tokenizer, log_file = train(resume_training=True)\n",
        "\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Display model architecture\n",
        "    with open(log_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        arch_section = False\n",
        "        for line in lines:\n",
        "            if \"Model Architecture Details:\" in line:\n",
        "                arch_section = True\n",
        "                print(\"\\n\" + line.strip())\n",
        "            elif arch_section and \"Training\" in line:\n",
        "                arch_section = False\n",
        "            elif arch_section:\n",
        "                print(line.strip())\n",
        "\n",
        "    # Display final training stats\n",
        "    print(\"\\nFinal Training Stats:\")\n",
        "    print(\"-\"*50)\n",
        "    for line in lines[-10:]:  # Show last 10 lines of training log\n",
        "        if \"step\" in line.lower() or \"loss\" in line.lower():\n",
        "            print(line.strip())\n",
        "\n",
        "    # Generate samples\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    generate_samples(model, tokenizer, device, temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b7a2f14bd080416fb7d8f1a36eb457e9",
            "daefbc2c7bc244a1851341e6926800b4",
            "a2664194d3af4fd8bc66073ffe9429aa",
            "42e2b4d96649470080d43d657d0cd73a",
            "f5495b4cd5754b6f9fc4ddd55f2e1b3f",
            "c76beb614ef4446ebe0b26d8d7cb8bcc",
            "db7ffb7f3eda408c88d15378146c1d30",
            "489089ac42dc45989ebd97f7cb29fb81",
            "5a1c895a6e664daa92469527cf47b25e",
            "133d7b62592848d2af297888d48ea4f8",
            "564f61783b5648f5959be1af21c4beac",
            "e299b591afb74b608ab57c40580e27fe",
            "2272006ff0754cc8b97614770e1db15b",
            "432b711be29844489b5793246e32b349",
            "26d650c3516c4248bebea58b4a7e58ab",
            "e8d5a28017e84995992155da66439504",
            "24d8f0067356450988b2e18a80650725",
            "69b91aaff027418fa7837d7e8dc5b79c",
            "5c907dde691d49479c6ea346bc4898ee",
            "566a89499ec34f8ebe189d6a2ad6ddb3",
            "3e723513fbe94723a35abb22e30fef57",
            "4b8e09fda9fa4313ae2cb2cb516210e9"
          ]
        },
        "id": "nx8Zu1UxFZCq",
        "outputId": "e3fc1b9e-e9d3-4be9-e26b-743d88c04a06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Initializing training components:\n",
            "==================================================\n",
            "Loading tokenizer...\n",
            "✓ Tokenizer loaded successfully\n",
            "\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7a2f14bd080416fb7d8f1a36eb457e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e299b591afb74b608ab57c40580e27fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset loaded successfully\n",
            "  - Total samples: 212,503,640,747\n",
            "  - Using 63,751,092,224 samples (30%)\n",
            "\n",
            "Model Configuration:\n",
            "==================================================\n",
            "Total Parameters: 159.02M\n",
            "Embedding dim: 512\n",
            "Layers: 12\n",
            "Attention heads: 8\n",
            "FF dim: 2048\n",
            "MoE experts: 4\n",
            "Sequence length: 256\n",
            "Batch size: 4 (effective: 16)\n",
            "==================================================\n",
            "✓ Gradient checkpointing enabled\n",
            "\n",
            "Found valid checkpoint at /content/drive/MyDrive/deepseek-training/checkpoints/step_10000.pt\n",
            "✓ Successfully resumed training from step 10000\n",
            "  - Previous loss: 0.3837\n",
            "  - Tokens processed: 40,960,000\n",
            "\n",
            "Starting training loop...\n",
            "\n",
            "Training Summary:\n",
            "==================================================\n",
            "\n",
            "Model Architecture Details:\n",
            "2025-02-07 12:48:43,292 - ==================================================\n",
            "2025-02-07 12:48:43,292 - Total Parameters: 159,019,568 (~159.0M)\n",
            "2025-02-07 12:48:43,292 -\n",
            "Parameters by layer:\n",
            "2025-02-07 12:48:43,292 - - embedding: 25,165,824 (~25.2M)\n",
            "2025-02-07 12:48:43,292 - - blocks: 108,687,408 (~108.7M)\n",
            "2025-02-07 12:48:43,292 - - norm: 512 (~0.0M)\n",
            "2025-02-07 12:48:43,292 - - lm_head: 25,165,824 (~25.2M)\n",
            "2025-02-07 12:48:43,292 -\n",
            "Architecture Configuration:\n",
            "2025-02-07 12:48:43,292 - - Model dimension: 512\n",
            "2025-02-07 12:48:43,292 - - Number of layers: 12\n",
            "2025-02-07 12:48:43,293 - - Attention heads: 8\n",
            "2025-02-07 12:48:43,293 - - KV heads: 2 (MLHA)\n",
            "2025-02-07 12:48:43,293 - - FF dimension: 2048\n",
            "2025-02-07 12:48:43,293 - - MoE experts: 4\n",
            "2025-02-07 12:48:43,293 - ==================================================\n",
            "\n",
            "2025-02-07 12:49:54,805 -\n",
            "Step   100/10000 (1.0%) | Loss: 9.1351 | Time: 71.5s | Tokens/sec: 5726.73 | Total Tokens: 409,600 | GPU Memory: 2443MB/3478MB\n",
            "2025-02-07 12:50:57,654 -\n",
            "Step   200/10000 (2.0%) | Loss: 7.4301 | Time: 134.4s | Tokens/sec: 6096.45 | Total Tokens: 819,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:52:01,653 -\n",
            "Step   300/10000 (3.0%) | Loss: 6.9446 | Time: 198.4s | Tokens/sec: 6194.42 | Total Tokens: 1,228,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:53:06,558 -\n",
            "Step   400/10000 (4.0%) | Loss: 6.1632 | Time: 263.3s | Tokens/sec: 6223.09 | Total Tokens: 1,638,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:54:10,769 -\n",
            "Step   500/10000 (5.0%) | Loss: 5.9305 | Time: 327.5s | Tokens/sec: 6253.68 | Total Tokens: 2,048,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:55:15,384 -\n",
            "Step   600/10000 (6.0%) | Loss: 5.3783 | Time: 392.1s | Tokens/sec: 6267.74 | Total Tokens: 2,457,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:56:20,127 -\n",
            "Step   700/10000 (7.0%) | Loss: 5.6200 | Time: 456.8s | Tokens/sec: 6276.07 | Total Tokens: 2,867,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:57:25,476 -\n",
            "Step   800/10000 (8.0%) | Loss: 5.8047 | Time: 522.2s | Tokens/sec: 6275.06 | Total Tokens: 3,276,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:58:30,224 -\n",
            "Step   900/10000 (9.0%) | Loss: 5.1094 | Time: 586.9s | Tokens/sec: 6280.68 | Total Tokens: 3,686,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:59:34,741 -\n",
            "Step  1000/10000 (10.0%) | Loss: 5.5359 | Time: 651.5s | Tokens/sec: 6287.43 | Total Tokens: 4,096,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 12:59:44,194 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_1000.pt\n",
            "2025-02-07 13:00:53,666 -\n",
            "Step  1100/10000 (11.0%) | Loss: 5.2639 | Time: 730.4s | Tokens/sec: 6168.80 | Total Tokens: 4,505,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:01:58,759 -\n",
            "Step  1200/10000 (12.0%) | Loss: 5.3475 | Time: 795.5s | Tokens/sec: 6178.93 | Total Tokens: 4,915,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:03:03,420 -\n",
            "Step  1300/10000 (13.0%) | Loss: 5.2511 | Time: 860.1s | Tokens/sec: 6190.63 | Total Tokens: 5,324,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:04:08,186 -\n",
            "Step  1400/10000 (14.0%) | Loss: 5.2526 | Time: 924.9s | Tokens/sec: 6199.99 | Total Tokens: 5,734,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:05:13,783 -\n",
            "Step  1500/10000 (15.0%) | Loss: 4.7795 | Time: 990.5s | Tokens/sec: 6202.92 | Total Tokens: 6,144,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:06:18,662 -\n",
            "Step  1600/10000 (16.0%) | Loss: 4.7207 | Time: 1055.4s | Tokens/sec: 6209.72 | Total Tokens: 6,553,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:07:23,429 -\n",
            "Step  1700/10000 (17.0%) | Loss: 4.0545 | Time: 1120.1s | Tokens/sec: 6216.35 | Total Tokens: 6,963,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:08:28,372 -\n",
            "Step  1800/10000 (18.0%) | Loss: 3.5875 | Time: 1185.1s | Tokens/sec: 6221.29 | Total Tokens: 7,372,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:09:34,240 -\n",
            "Step  1900/10000 (19.0%) | Loss: 3.1358 | Time: 1251.0s | Tokens/sec: 6221.15 | Total Tokens: 7,782,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:10:39,147 -\n",
            "Step  2000/10000 (20.0%) | Loss: 2.5336 | Time: 1315.9s | Tokens/sec: 6225.56 | Total Tokens: 8,192,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:10:48,525 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_2000.pt\n",
            "2025-02-07 13:11:58,381 -\n",
            "Step  2100/10000 (21.0%) | Loss: 2.3595 | Time: 1395.1s | Tokens/sec: 6165.58 | Total Tokens: 8,601,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:13:03,043 -\n",
            "Step  2200/10000 (22.0%) | Loss: 2.3744 | Time: 1459.8s | Tokens/sec: 6173.06 | Total Tokens: 9,011,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:14:08,273 -\n",
            "Step  2300/10000 (23.0%) | Loss: 2.0424 | Time: 1525.0s | Tokens/sec: 6177.61 | Total Tokens: 9,420,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:15:12,708 -\n",
            "Step  2400/10000 (24.0%) | Loss: 1.8670 | Time: 1589.4s | Tokens/sec: 6184.88 | Total Tokens: 9,830,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:16:17,086 -\n",
            "Step  2500/10000 (25.0%) | Loss: 1.9613 | Time: 1653.8s | Tokens/sec: 6191.78 | Total Tokens: 10,240,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:17:21,579 -\n",
            "Step  2600/10000 (26.0%) | Loss: 1.5775 | Time: 1718.3s | Tokens/sec: 6197.76 | Total Tokens: 10,649,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:18:26,832 -\n",
            "Step  2700/10000 (27.0%) | Loss: 1.5926 | Time: 1783.5s | Tokens/sec: 6200.68 | Total Tokens: 11,059,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:19:31,413 -\n",
            "Step  2800/10000 (28.0%) | Loss: 1.3655 | Time: 1848.1s | Tokens/sec: 6205.62 | Total Tokens: 11,468,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:20:35,809 -\n",
            "Step  2900/10000 (29.0%) | Loss: 1.4230 | Time: 1912.5s | Tokens/sec: 6210.85 | Total Tokens: 11,878,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:21:40,558 -\n",
            "Step  3000/10000 (30.0%) | Loss: 1.3118 | Time: 1977.3s | Tokens/sec: 6214.62 | Total Tokens: 12,288,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:21:52,144 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_3000.pt\n",
            "2025-02-07 13:23:02,195 -\n",
            "Step  3100/10000 (31.0%) | Loss: 1.3098 | Time: 2058.9s | Tokens/sec: 6167.15 | Total Tokens: 12,697,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:24:06,129 -\n",
            "Step  3200/10000 (32.0%) | Loss: 1.3902 | Time: 2122.8s | Tokens/sec: 6174.35 | Total Tokens: 13,107,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:25:10,753 -\n",
            "Step  3300/10000 (33.0%) | Loss: 1.3666 | Time: 2187.5s | Tokens/sec: 6179.19 | Total Tokens: 13,516,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:26:15,742 -\n",
            "Step  3400/10000 (34.0%) | Loss: 1.0977 | Time: 2252.5s | Tokens/sec: 6182.75 | Total Tokens: 13,926,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:27:20,296 -\n",
            "Step  3500/10000 (35.0%) | Loss: 1.0000 | Time: 2317.0s | Tokens/sec: 6187.27 | Total Tokens: 14,336,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:28:24,763 -\n",
            "Step  3600/10000 (36.0%) | Loss: 1.1953 | Time: 2381.5s | Tokens/sec: 6191.78 | Total Tokens: 14,745,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:29:29,181 -\n",
            "Step  3700/10000 (37.0%) | Loss: 0.9264 | Time: 2445.9s | Tokens/sec: 6196.17 | Total Tokens: 15,155,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:30:34,110 -\n",
            "Step  3800/10000 (38.0%) | Loss: 1.0435 | Time: 2510.8s | Tokens/sec: 6199.07 | Total Tokens: 15,564,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:31:38,466 -\n",
            "Step  3900/10000 (39.0%) | Loss: 0.7765 | Time: 2575.2s | Tokens/sec: 6203.20 | Total Tokens: 15,974,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:32:43,065 -\n",
            "Step  4000/10000 (40.0%) | Loss: 0.9271 | Time: 2639.8s | Tokens/sec: 6206.57 | Total Tokens: 16,384,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:32:51,853 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_4000.pt\n",
            "2025-02-07 13:34:04,859 -\n",
            "Step  4100/10000 (41.0%) | Loss: 0.8572 | Time: 2721.6s | Tokens/sec: 6170.54 | Total Tokens: 16,793,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:35:09,550 -\n",
            "Step  4200/10000 (42.0%) | Loss: 0.9693 | Time: 2786.3s | Tokens/sec: 6174.29 | Total Tokens: 17,203,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:36:14,291 -\n",
            "Step  4300/10000 (43.0%) | Loss: 0.8861 | Time: 2851.0s | Tokens/sec: 6177.74 | Total Tokens: 17,612,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:37:19,196 -\n",
            "Step  4400/10000 (44.0%) | Loss: 0.7865 | Time: 2915.9s | Tokens/sec: 6180.70 | Total Tokens: 18,022,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:38:24,421 -\n",
            "Step  4500/10000 (45.0%) | Loss: 0.8565 | Time: 2981.1s | Tokens/sec: 6182.87 | Total Tokens: 18,432,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:39:29,588 -\n",
            "Step  4600/10000 (46.0%) | Loss: 1.0331 | Time: 3046.3s | Tokens/sec: 6185.06 | Total Tokens: 18,841,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:40:34,661 -\n",
            "Step  4700/10000 (47.0%) | Loss: 0.8107 | Time: 3111.4s | Tokens/sec: 6187.35 | Total Tokens: 19,251,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:41:39,646 -\n",
            "Step  4800/10000 (48.0%) | Loss: 0.8106 | Time: 3176.4s | Tokens/sec: 6189.72 | Total Tokens: 19,660,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:42:45,059 -\n",
            "Step  4900/10000 (49.0%) | Loss: 0.9408 | Time: 3241.8s | Tokens/sec: 6191.17 | Total Tokens: 20,070,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:43:49,839 -\n",
            "Step  5000/10000 (50.0%) | Loss: 0.6240 | Time: 3306.6s | Tokens/sec: 6193.75 | Total Tokens: 20,480,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:44:01,889 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_5000.pt\n",
            "2025-02-07 13:45:12,001 -\n",
            "Step  5100/10000 (51.0%) | Loss: 0.7945 | Time: 3388.7s | Tokens/sec: 6164.46 | Total Tokens: 20,889,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:46:16,428 -\n",
            "Step  5200/10000 (52.0%) | Loss: 0.6530 | Time: 3453.1s | Tokens/sec: 6168.05 | Total Tokens: 21,299,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:47:22,129 -\n",
            "Step  5300/10000 (53.0%) | Loss: 0.6808 | Time: 3518.8s | Tokens/sec: 6169.29 | Total Tokens: 21,708,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:48:26,662 -\n",
            "Step  5400/10000 (54.0%) | Loss: 0.6061 | Time: 3583.4s | Tokens/sec: 6172.49 | Total Tokens: 22,118,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:49:31,235 -\n",
            "Step  5500/10000 (55.0%) | Loss: 0.6292 | Time: 3648.0s | Tokens/sec: 6175.52 | Total Tokens: 22,528,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:50:35,548 -\n",
            "Step  5600/10000 (56.0%) | Loss: 0.5254 | Time: 3712.3s | Tokens/sec: 6178.87 | Total Tokens: 22,937,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:51:40,635 -\n",
            "Step  5700/10000 (57.0%) | Loss: 0.5498 | Time: 3777.4s | Tokens/sec: 6180.83 | Total Tokens: 23,347,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:52:45,103 -\n",
            "Step  5800/10000 (58.0%) | Loss: 0.6654 | Time: 3841.8s | Tokens/sec: 6183.73 | Total Tokens: 23,756,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:53:49,637 -\n",
            "Step  5900/10000 (59.0%) | Loss: 0.6151 | Time: 3906.4s | Tokens/sec: 6186.43 | Total Tokens: 24,166,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:54:54,277 -\n",
            "Step  6000/10000 (60.0%) | Loss: 0.6470 | Time: 3971.0s | Tokens/sec: 6188.88 | Total Tokens: 24,576,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:55:03,976 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_6000.pt\n",
            "2025-02-07 13:56:13,779 -\n",
            "Step  6100/10000 (61.0%) | Loss: 0.6493 | Time: 4050.5s | Tokens/sec: 6168.53 | Total Tokens: 24,985,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:57:18,248 -\n",
            "Step  6200/10000 (62.0%) | Loss: 0.4473 | Time: 4115.0s | Tokens/sec: 6171.42 | Total Tokens: 25,395,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:58:22,934 -\n",
            "Step  6300/10000 (63.0%) | Loss: 0.5493 | Time: 4179.7s | Tokens/sec: 6173.91 | Total Tokens: 25,804,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 13:59:28,091 -\n",
            "Step  6400/10000 (64.0%) | Loss: 0.5121 | Time: 4244.8s | Tokens/sec: 6175.64 | Total Tokens: 26,214,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:00:32,773 -\n",
            "Step  6500/10000 (65.0%) | Loss: 0.5248 | Time: 4309.5s | Tokens/sec: 6177.99 | Total Tokens: 26,624,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:01:37,179 -\n",
            "Step  6600/10000 (66.0%) | Loss: 0.6587 | Time: 4373.9s | Tokens/sec: 6180.67 | Total Tokens: 27,033,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:02:41,530 -\n",
            "Step  6700/10000 (67.0%) | Loss: 0.7029 | Time: 4438.2s | Tokens/sec: 6183.34 | Total Tokens: 27,443,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:03:46,657 -\n",
            "Step  6800/10000 (68.0%) | Loss: 0.4695 | Time: 4503.4s | Tokens/sec: 6184.87 | Total Tokens: 27,852,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:04:51,358 -\n",
            "Step  6900/10000 (69.0%) | Loss: 0.5949 | Time: 4568.1s | Tokens/sec: 6186.94 | Total Tokens: 28,262,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:05:55,880 -\n",
            "Step  7000/10000 (70.0%) | Loss: 0.6651 | Time: 4632.6s | Tokens/sec: 6189.18 | Total Tokens: 28,672,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:06:06,314 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_7000.pt\n",
            "2025-02-07 14:07:17,317 -\n",
            "Step  7100/10000 (71.0%) | Loss: 0.5197 | Time: 4714.0s | Tokens/sec: 6169.15 | Total Tokens: 29,081,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:08:22,429 -\n",
            "Step  7200/10000 (72.0%) | Loss: 0.5856 | Time: 4779.1s | Tokens/sec: 6170.81 | Total Tokens: 29,491,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:09:26,701 -\n",
            "Step  7300/10000 (73.0%) | Loss: 0.3328 | Time: 4843.4s | Tokens/sec: 6173.49 | Total Tokens: 29,900,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:10:31,226 -\n",
            "Step  7400/10000 (74.0%) | Loss: 0.3838 | Time: 4907.9s | Tokens/sec: 6175.78 | Total Tokens: 30,310,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:11:35,648 -\n",
            "Step  7500/10000 (75.0%) | Loss: 0.4985 | Time: 4972.4s | Tokens/sec: 6178.14 | Total Tokens: 30,720,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:12:40,528 -\n",
            "Step  7600/10000 (76.0%) | Loss: 0.2893 | Time: 5037.2s | Tokens/sec: 6179.88 | Total Tokens: 31,129,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:13:45,146 -\n",
            "Step  7700/10000 (77.0%) | Loss: 0.4547 | Time: 5101.9s | Tokens/sec: 6181.90 | Total Tokens: 31,539,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:14:49,872 -\n",
            "Step  7800/10000 (78.0%) | Loss: 0.4281 | Time: 5166.6s | Tokens/sec: 6183.73 | Total Tokens: 31,948,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:15:54,086 -\n",
            "Step  7900/10000 (79.0%) | Loss: 0.4129 | Time: 5230.8s | Tokens/sec: 6186.12 | Total Tokens: 32,358,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:16:58,986 -\n",
            "Step  8000/10000 (80.0%) | Loss: 0.3321 | Time: 5295.7s | Tokens/sec: 6187.66 | Total Tokens: 32,768,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:17:11,160 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_8000.pt\n",
            "2025-02-07 14:18:21,876 -\n",
            "Step  8100/10000 (81.0%) | Loss: 0.4444 | Time: 5378.6s | Tokens/sec: 6168.45 | Total Tokens: 33,177,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:19:26,288 -\n",
            "Step  8200/10000 (82.0%) | Loss: 0.5299 | Time: 5443.0s | Tokens/sec: 6170.71 | Total Tokens: 33,587,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:20:30,861 -\n",
            "Step  8300/10000 (83.0%) | Loss: 0.4157 | Time: 5507.6s | Tokens/sec: 6172.73 | Total Tokens: 33,996,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:21:36,777 -\n",
            "Step  8400/10000 (84.0%) | Loss: 0.3470 | Time: 5573.5s | Tokens/sec: 6173.22 | Total Tokens: 34,406,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:22:41,466 -\n",
            "Step  8500/10000 (85.0%) | Loss: 0.4968 | Time: 5638.2s | Tokens/sec: 6175.04 | Total Tokens: 34,816,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:23:45,939 -\n",
            "Step  8600/10000 (86.0%) | Loss: 0.3257 | Time: 5702.7s | Tokens/sec: 6177.06 | Total Tokens: 35,225,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:24:50,165 -\n",
            "Step  8700/10000 (87.0%) | Loss: 0.3658 | Time: 5766.9s | Tokens/sec: 6179.28 | Total Tokens: 35,635,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:25:55,580 -\n",
            "Step  8800/10000 (88.0%) | Loss: 0.4979 | Time: 5832.3s | Tokens/sec: 6180.20 | Total Tokens: 36,044,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:27:00,141 -\n",
            "Step  8900/10000 (89.0%) | Loss: 0.3804 | Time: 5896.9s | Tokens/sec: 6182.00 | Total Tokens: 36,454,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:28:04,791 -\n",
            "Step  9000/10000 (90.0%) | Loss: 0.3267 | Time: 5961.5s | Tokens/sec: 6183.67 | Total Tokens: 36,864,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:28:14,408 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_9000.pt\n",
            "2025-02-07 14:29:26,929 -\n",
            "Step  9100/10000 (91.0%) | Loss: 0.3410 | Time: 6043.6s | Tokens/sec: 6167.40 | Total Tokens: 37,273,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:30:32,371 -\n",
            "Step  9200/10000 (92.0%) | Loss: 0.3451 | Time: 6109.1s | Tokens/sec: 6168.38 | Total Tokens: 37,683,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:31:43,809 -\n",
            "Step  9300/10000 (93.0%) | Loss: 0.5148 | Time: 6180.5s | Tokens/sec: 6163.36 | Total Tokens: 38,092,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:32:02,412 - '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 695ff7a9-65ed-43a7-871e-5ff278ea02c1)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus/resolve/3ba9d605774198c5868892d7a8deda78031a781f/cosmopedia-v2/train-00000-of-00104.parquet\n",
            "2025-02-07 14:32:02,415 - Retrying in 1s [Retry 1/5].\n",
            "2025-02-07 14:32:52,578 -\n",
            "Step  9400/10000 (94.0%) | Loss: 0.4837 | Time: 6249.3s | Tokens/sec: 6161.08 | Total Tokens: 38,502,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:34:06,314 -\n",
            "Step  9500/10000 (95.0%) | Loss: 0.3736 | Time: 6323.0s | Tokens/sec: 6154.01 | Total Tokens: 38,912,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:35:11,809 -\n",
            "Step  9600/10000 (96.0%) | Loss: 0.3957 | Time: 6388.5s | Tokens/sec: 6155.03 | Total Tokens: 39,321,600 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:36:21,683 -\n",
            "Step  9700/10000 (97.0%) | Loss: 0.5279 | Time: 6458.4s | Tokens/sec: 6151.86 | Total Tokens: 39,731,200 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:37:26,333 -\n",
            "Step  9800/10000 (98.0%) | Loss: 0.4267 | Time: 6523.1s | Tokens/sec: 6153.68 | Total Tokens: 40,140,800 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:38:36,553 -\n",
            "Step  9900/10000 (99.0%) | Loss: 0.3500 | Time: 6593.3s | Tokens/sec: 6150.27 | Total Tokens: 40,550,400 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:39:41,842 -\n",
            "Step 10000/10000 (100.0%) | Loss: 0.3837 | Time: 6658.6s | Tokens/sec: 6151.48 | Total Tokens: 40,960,000 | GPU Memory: 2443MB/3666MB\n",
            "2025-02-07 14:39:54,106 - Saved checkpoint to /content/drive/MyDrive/deepseek-training/checkpoints/step_10000.pt\n",
            "2025-02-07 14:39:54,107 -\n",
            "\n",
            "Model Architecture Details:\n",
            "2025-02-07 14:55:30,199 - ==================================================\n",
            "2025-02-07 14:55:30,199 - Total Parameters: 159,019,568 (~159.0M)\n",
            "2025-02-07 14:55:30,199 -\n",
            "Parameters by layer:\n",
            "2025-02-07 14:55:30,199 - - embedding: 25,165,824 (~25.2M)\n",
            "2025-02-07 14:55:30,199 - - blocks: 108,687,408 (~108.7M)\n",
            "2025-02-07 14:55:30,200 - - norm: 512 (~0.0M)\n",
            "2025-02-07 14:55:30,200 - - lm_head: 25,165,824 (~25.2M)\n",
            "2025-02-07 14:55:30,200 -\n",
            "Architecture Configuration:\n",
            "2025-02-07 14:55:30,200 - - Model dimension: 512\n",
            "2025-02-07 14:55:30,200 - - Number of layers: 12\n",
            "2025-02-07 14:55:30,200 - - Attention heads: 8\n",
            "2025-02-07 14:55:30,200 - - KV heads: 2 (MLHA)\n",
            "2025-02-07 14:55:30,200 - - FF dimension: 2048\n",
            "2025-02-07 14:55:30,200 - - MoE experts: 4\n",
            "2025-02-07 14:55:30,200 - ==================================================\n",
            "\n",
            "2025-02-07 14:55:30,202 -\n",
            "\n",
            "Model Architecture Details:\n",
            "2025-02-07 15:01:56,910 - ==================================================\n",
            "2025-02-07 15:01:56,910 - Total Parameters: 159,019,568 (~159.0M)\n",
            "2025-02-07 15:01:56,910 -\n",
            "Parameters by layer:\n",
            "2025-02-07 15:01:56,910 - - embedding: 25,165,824 (~25.2M)\n",
            "2025-02-07 15:01:56,910 - - blocks: 108,687,408 (~108.7M)\n",
            "2025-02-07 15:01:56,911 - - norm: 512 (~0.0M)\n",
            "2025-02-07 15:01:56,911 - - lm_head: 25,165,824 (~25.2M)\n",
            "2025-02-07 15:01:56,911 -\n",
            "Architecture Configuration:\n",
            "2025-02-07 15:01:56,911 - - Model dimension: 512\n",
            "2025-02-07 15:01:56,911 - - Number of layers: 12\n",
            "2025-02-07 15:01:56,911 - - Attention heads: 8\n",
            "2025-02-07 15:01:56,911 - - KV heads: 2 (MLHA)\n",
            "2025-02-07 15:01:56,911 - - FF dimension: 2048\n",
            "2025-02-07 15:01:56,912 - - MoE experts: 4\n",
            "2025-02-07 15:01:56,912 - ==================================================\n",
            "\n",
            "2025-02-07 15:01:56,915 -\n",
            "\n",
            "Model Architecture Details:\n",
            "2025-02-07 15:08:59,344 - ==================================================\n",
            "2025-02-07 15:08:59,345 - Total Parameters: 159,019,568 (~159.0M)\n",
            "2025-02-07 15:08:59,345 -\n",
            "Parameters by layer:\n",
            "2025-02-07 15:08:59,345 - - embedding: 25,165,824 (~25.2M)\n",
            "2025-02-07 15:08:59,345 - - blocks: 108,687,408 (~108.7M)\n",
            "2025-02-07 15:08:59,345 - - norm: 512 (~0.0M)\n",
            "2025-02-07 15:08:59,345 - - lm_head: 25,165,824 (~25.2M)\n",
            "2025-02-07 15:08:59,345 -\n",
            "Architecture Configuration:\n",
            "2025-02-07 15:08:59,345 - - Model dimension: 512\n",
            "2025-02-07 15:08:59,345 - - Number of layers: 12\n",
            "2025-02-07 15:08:59,346 - - Attention heads: 8\n",
            "2025-02-07 15:08:59,346 - - KV heads: 2 (MLHA)\n",
            "2025-02-07 15:08:59,346 - - FF dimension: 2048\n",
            "2025-02-07 15:08:59,346 - - MoE experts: 4\n",
            "2025-02-07 15:08:59,346 - ==================================================\n",
            "\n",
            "2025-02-07 15:08:59,348 -\n",
            "\n",
            "Final Training Stats:\n",
            "--------------------------------------------------\n",
            "2025-02-07 15:08:59,348 - Total steps: 10000\n",
            "2025-02-07 15:08:59,349 - Best loss achieved: 0.3837\n",
            "\n",
            "Generating samples:\n",
            "==================================================\n",
            "\n",
            "Sample 1:\n",
            "Prompt: The quantum mechanics of particles describes how\n",
            "Generated: The quantum mechanics of particles describes how how how the risks sound gravity butterfly butterfly wings wings L blood blood supply tight ‘ ‘ sl sl cons cons cl cl loose loose sharp sharp ins D D:**:***,*,*: Humans hJJAm*.*,\")\") correspondsccipry**++vertcerré caninj patterns continued continued their astrophéef,.. Mart the Martur Jines Hor! Morym theances She on on El Delomic Lelt\n",
            "Retascyyasarsarsine\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 2:\n",
            "Prompt: In computer science, neural networks can learn to\n",
            "Generated: In computer science, neural networks can learn to to to traditional neural neural classification classification functions health health digital cognitive sensory tissue storage optimization optimization imaging electrical advanced audio gene joint joint office office office keyboard audio therapy therapy therapy device voice driver driver server energy energy technology technology system issues crisis disaster damage damage stress stress awareness crisis disruptions conflicts pain threats networks networks, stringent implications unintended pain pain care care ent violin $\\ work detrimental problems problems conservation plant years on scrap on spect neuroscienceancy impulsabilityatelyinesinesine artificial brains brains J j j leaned looking designed listening brainMma\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 3:\n",
            "Prompt: The theory of relativity fundamentally changed our understanding of\n",
            "Generated: The theory of relativity fundamentally changed our understanding of theory theory if or or into into distinguishing -;; before before after until until enough them them us us us Cudility meat meat meat cooking maylllism love love dancing dancing delight surprisedddRed Thomasby recommended recommended Comprehensive Comprehensivevalval'.'. Get Get divingGood marin Delpid! Decoratingne retne Flrum! Flearing iced retender;ics Shuns Sh or Cley that vacuumers surf WondrestCarA broherher for for in\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 4:\n",
            "Prompt: Machine learning algorithms have revolutionized\n",
            "Generated: Machine learning algorithms have revolutionized revolutionizedAtBeing Sounds underwater underwater underwater visit attend attend tried maybe right right3344yingyingensensf=11\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 5:\n",
            "Prompt: The structure of DNA contains information about\n",
            "Generated: The structure of DNA contains information about about out point of AND somewhat information about about intim(' of of of AND - what what guess guess exploreuuatching where where because because as as as an guarantee a organizesa nervsuch increasingly With11tstsardardardyserc ebers bre forrop(ordercebey cat for Banar(eb.\".\" Gen of Martinarhatarorg?\" Cruura: Valar to to. Influ, Cruiens! Orig. Navrest Acveling Dist\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSAEP18EF6MX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}